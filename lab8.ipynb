{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21969328",
   "metadata": {},
   "source": [
    "# Laboratorio 8 - Defensa contra ataques de modelos de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19e9bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn; sn.set(font_scale=1.4)\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb5f1d4-47c5-4fc1-8805-e8525a4eb888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    DIRECTORY = os.getcwd()\n",
    "    DIRECTORY = os.path.join(DIRECTORY, \"malimg_paper_dataset_imgs\")\n",
    "    class_names = []\n",
    "    labels = []\n",
    "    images = []\n",
    "    i = 0\n",
    "    example_images = []\n",
    "    for folder in os.listdir(DIRECTORY):\n",
    "        folder_directory = os.path.join(DIRECTORY, folder)\n",
    "        if not os.path.isdir(folder_directory): continue\n",
    "        class_names.append(folder)\n",
    "        \n",
    "        example_set = False\n",
    "        \n",
    "        for file in os.listdir(folder_directory):\n",
    "            img_path = os.path.join(folder_directory, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (150, 150))\n",
    "            labels.append(i)\n",
    "            images.append(image)\n",
    "            if not example_set: \n",
    "                example_images.append(image)\n",
    "                example_set = True\n",
    "        i += 1\n",
    "    \n",
    "    images = np.array(images, dtype = 'float32')\n",
    "    labels = np.array(labels, dtype = 'int32')\n",
    "    \n",
    "    return class_names, images, labels, example_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7ee3b4-11cf-46cb-8e46-2b4afb28ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names, images, labels, example_images = load_data()\n",
    "images, labels = shuffle(images, labels, random_state=123)\n",
    "images = images.reshape(len(images), 150, 150, 1)\n",
    "attack_images = images[:1000]\n",
    "attack_labels = labels[:1000]\n",
    "images = images[1000:]\n",
    "labels = labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d31b46-ca27-4bce-a5fe-7a765ea2ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint = int(len(images)*0.7)\n",
    "train_images = images[:breakpoint]\n",
    "test_images = images[breakpoint:]\n",
    "train_labels = labels[:breakpoint]\n",
    "test_labels = labels[breakpoint:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60562cec-cc45-43ba-b4ae-aec371032132",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation=tf.nn.relu),\n",
    "    Dense(len(class_names), activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8429d57b-c4ec-4b38-a66c-534a8e0480f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da571f9-664b-4f5a-9847-1bf7e8a77678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4669 samples, validate on 1168 samples\n",
      "Epoch 1/6\n",
      "4669/4669 [==============================] - ETA: 0s - loss: 44.1899 - accuracy: 0.5194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 44.1899 - accuracy: 0.5194 - val_loss: 0.5092 - val_accuracy: 0.8853\n",
      "Epoch 2/6\n",
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 0.5308 - accuracy: 0.9154 - val_loss: 0.4652 - val_accuracy: 0.9067\n",
      "Epoch 3/6\n",
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 0.1247 - accuracy: 0.9717 - val_loss: 0.2707 - val_accuracy: 0.9426\n",
      "Epoch 4/6\n",
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 0.0106 - accuracy: 0.9976 - val_loss: 0.2228 - val_accuracy: 0.9598\n",
      "Epoch 5/6\n",
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.2251 - val_accuracy: 0.9443\n",
      "Epoch 6/6\n",
      "4669/4669 [==============================] - 8s 2ms/sample - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2127 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f168c635d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.fit(train_images, train_labels, batch_size=128, epochs=6, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d677709-4ba8-4d92-947d-57586b4db8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(\n",
    "    model=target_model,\n",
    "    clip_values=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98d14f-f7b3-4dd8-870b-d6ec0c760f41",
   "metadata": {},
   "source": [
    "## Defensa de ataque adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "134b630c-ffab-4da8-82f7-b2493052f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = FastGradientMethod(\n",
    "    estimator=classifier,\n",
    "    eps=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a459509b-1c1a-459e-9587-ac9ac97430cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_adv = attack.generate(x=attack_images)\n",
    "train_images_adv = images_adv[:len(images_adv)//2]\n",
    "train_labels_adv = attack_labels[:len(images_adv)//2]\n",
    "test_images_adv = images_adv[len(images_adv)//2:]\n",
    "test_images_noadv = attack_images[len(attack_images)//2:]\n",
    "test_labels_adv = attack_labels[len(images_adv)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90fb682d-0913-4d9e-a390-ef450b4b3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(32, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation=tf.nn.relu),\n",
    "    Dense(len(class_names), activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26a15648-360f-4827-978b-4c1547f1dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae69ca0e-b8eb-4aec-901b-9e8c427f3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "defense_images = np.append(train_images, train_images_adv, 0)\n",
    "defense_labels = np.append(train_labels, train_labels_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03a4bd44-bb4f-4a1d-8150-97fdfb868e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5069 samples, validate on 1268 samples\n",
      "Epoch 1/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 59.7257 - accuracy: 0.5068 - val_loss: 0.8222 - val_accuracy: 0.8423\n",
      "Epoch 2/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 0.3357 - accuracy: 0.9235 - val_loss: 0.2877 - val_accuracy: 0.9306\n",
      "Epoch 3/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 0.0648 - accuracy: 0.9795 - val_loss: 0.2510 - val_accuracy: 0.9543\n",
      "Epoch 4/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 0.0234 - accuracy: 0.9947 - val_loss: 0.2592 - val_accuracy: 0.9535\n",
      "Epoch 5/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 0.0084 - accuracy: 0.9992 - val_loss: 0.2637 - val_accuracy: 0.9550\n",
      "Epoch 6/6\n",
      "5069/5069 [==============================] - 8s 2ms/sample - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.2532 - val_accuracy: 0.9558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15be58d2b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(defense_images, defense_labels, batch_size=128, epochs=6, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5cf8133-2ad3-42ff-b4ca-f707bc73c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean test set loss: 0.17 vs adversarial set test loss: 0.17\n",
      "Clean test set accuracy: 0.97 vs adversarial test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluating the model on clean images\n",
    "score_clean = model2.evaluate(\n",
    "    x=test_images_noadv, \n",
    "    y=test_labels_adv\n",
    "    )\n",
    "\n",
    "# Evaluating the model on adversarial images\n",
    "score_adv = model2.evaluate(\n",
    "    x=test_images_adv, \n",
    "    y=test_labels_adv\n",
    "    )\n",
    "\n",
    "# Comparing test losses\n",
    "print(f\"Clean test set loss: {score_clean[0]:.2f} \" \n",
    "      f\"vs adversarial set test loss: {score_adv[0]:.2f}\")\n",
    "\n",
    "# Comparing test accuracies\n",
    "print(f\"Clean test set accuracy: {score_clean[1]:.2f} \" \n",
    "      f\"vs adversarial test set accuracy: {score_adv[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42c1ea-3e6b-4ae4-89b2-fe8b1f10e8ec",
   "metadata": {},
   "source": [
    "Como podemos ver el model2 (que fue entrenado con las imagenes adversarias train) puede predecir con una accuracy de 97% las imágenes limpias y también las imágenes alteradas por lo que la defensa funcionó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9060e2-1bb7-4b72-a1bc-31c0116643d6",
   "metadata": {},
   "source": [
    "## Defensa de ataque adversarial con art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecfc4162-9a98-49c8-b3d0-606a223261e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = AdversarialTrainer(classifier=classifier, attacks=[attack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d244dd20-0eda-4a32-9319-2b4e19f7e9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0333b0fad88e47298f5638a10409876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precompute adv samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecd70fad51c47e0aa322d22f35b3cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adversarial training epochs:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(train_images, train_labels, nb_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a92033a2-947c-491a-b888-2e83dd4102ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_adv = detector.detect(x=detection_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfb79073-bd27-4d66-a517-7f060a40b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_images)\n",
    "pred_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "08834929-bbc5-4577-b8bd-80c076b9ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_adv = trainer.predict(images_adv)\n",
    "pred_labels_adv = np.argmax(predictions_adv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd09af44-867b-4f3c-87a3-e503376caca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy clean: 0.95, Accuracy adv: 0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy clean: %.2f, Accuracy adv: %.2f\"%(accuracy_score(test_labels, pred_labels), accuracy_score(attack_labels, pred_labels_adv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32516ec-09bb-4323-962f-d387dac1af93",
   "metadata": {},
   "source": [
    "Como podemos ver, en este modelo entrenado con imágenes adversariales usando art el accuracy se sigue manteniendo aunque se usen imágenes adversariales o imágenes limpias, incluso aumentó ligeramente de 95% a 96%. Esto nos indica que la defensa funcionó correctamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-kernel",
   "language": "python",
   "name": "tf-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
